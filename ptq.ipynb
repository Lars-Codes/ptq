{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training YoloNAS on custom dataset, applying post-training quantization, converting to ONNX, and then OpenVINO IR & doing inference on that \n",
    "https://github.com/Deci-AI/super-gradients/blob/master/notebooks/yolo_nas_custom_dataset_fine_tuning_with_qat.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qq super-gradients==3.7.1 datasets[vision]~=2.1 pandas~=2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Remember what class names you labeled your images with. \n",
    "    If you assigned labels of “truck” and “car” for example, replace \n",
    "    the CLASS_NAMES line with CLASS_NAMES = [“truck”, “car”]\n",
    "\"\"\"\n",
    "CLASS_NAMES = ['vehicle'] # Update with an array of class names \n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:38:32] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /home/lpalombi/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:38:35] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: protobuf==3.20.2 does not satisfy requirement protobuf==3.20.3\u001b[0m\n",
      "[2024-07-11 11:38:35] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
      "Indexing dataset annotations: 100%|██████████| 441/441 [00:00<00:00, 7207.49it/s]\n",
      "[2024-07-11 11:38:35] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing dataset annotations: 100%|██████████| 111/111 [00:00<00:00, 10115.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The program will concatenate data_dir/images_dir and data_dir/json_annotation_file. \n",
    "These paths are local to the .ipynb train file you are using. \n",
    "\n",
    "My project structure was as follows: \n",
    "ROOT \n",
    "    -Train.ipynb \n",
    "    -Vehicle_images\n",
    "        -Image1.png \n",
    "        -Img2.png \n",
    "        -…. ImgX.png \n",
    "        -Train.json \n",
    "        -Val.json \n",
    "\n",
    "So, therefore, for me: \n",
    "data_dir =“vehicle_images”\n",
    "images_dir=””\n",
    "json_annotation_file=”train.json” \n",
    "\n",
    "If necessary, in dataset_params for train and val, update the 3 locations within the “transforms” parameter where \n",
    "it says target_size and input_dim to match your models input dimensions in (H, W) format. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from super_gradients.training.datasets.detection_datasets.coco_format_detection import COCOFormatDetectionDataset\n",
    "from super_gradients.training.transforms.transforms import (\n",
    "    DetectionRandomAffine,\n",
    "    DetectionHSV,\n",
    "    DetectionHorizontalFlip,\n",
    "    DetectionPaddedRescale,\n",
    "    DetectionStandardize,\n",
    "    DetectionTargetsFormatTransform,\n",
    ")\n",
    "from super_gradients.training.utils.collate_fn import DetectionCollateFN\n",
    "\n",
    "train_dataset_params = dict(\n",
    "    data_dir=\"vehicle_images\", # local path to directory that contains both images and json annotation file\n",
    "    images_dir=\"\", # Local path FROM DATA DIR to where images are located (data_dir/path/to/images)\n",
    "    json_annotation_file=\"train.json\", # Local path FROM DATA DIR to where train.json is located\n",
    "    input_dim=(1920, 1088),\n",
    "    ignore_empty_annotations=False,\n",
    "    with_crowd=False,\n",
    "    all_classes_list=CLASS_NAMES,\n",
    "    transforms=[\n",
    "        DetectionRandomAffine(degrees=0.0, scales=(0.5, 1.5), shear=0.0, target_size=(640, 640), filter_box_candidates=False, border_value=128), # update target_size if necessary\n",
    "        DetectionHSV(prob=1.0, hgain=5, vgain=30, sgain=30),\n",
    "        DetectionHorizontalFlip(prob=0.5),\n",
    "        DetectionPaddedRescale(input_dim=(640, 640)), # update \n",
    "        DetectionStandardize(max_value=255),\n",
    "        DetectionTargetsFormatTransform(input_dim=(640, 640), output_format=\"LABEL_CXCYWH\"), # update input_dim \n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# make the same changes above here \n",
    "valid_dataset_params = dict(\n",
    "    data_dir=\"vehicle_images\", \n",
    "    images_dir=\"\",\n",
    "    json_annotation_file=\"val.json\", # val.json, not train.json \n",
    "    input_dim=(640, 640),\n",
    "    ignore_empty_annotations=False,\n",
    "    with_crowd=False,\n",
    "    all_classes_list=CLASS_NAMES,\n",
    "    transforms=[\n",
    "        DetectionPaddedRescale(input_dim=(640, 640), max_targets=300),\n",
    "        DetectionStandardize(max_value=255),\n",
    "        DetectionTargetsFormatTransform(input_dim=(640, 640), output_format=\"LABEL_CXCYWH\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainset = COCOFormatDetectionDataset(**train_dataset_params)\n",
    "valset = COCOFormatDetectionDataset(**valid_dataset_params)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 1 # update batch size \n",
    "\n",
    "train_dataloader_params = {\n",
    "    \"shuffle\": True,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"drop_last\": True,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": DetectionCollateFN(),\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"persistent_workers\": NUM_WORKERS > 0,\n",
    "}\n",
    "\n",
    "val_dataloader_params = {\n",
    "    \"shuffle\": False,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"drop_last\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": DetectionCollateFN(),\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"persistent_workers\": NUM_WORKERS > 0,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(trainset, **train_dataloader_params)\n",
    "valid_loader = DataLoader(valset, **val_dataloader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import DetectionMetrics_050\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback\n",
    "\n",
    "\"\"\"\n",
    "    From the original tutorial I updated the following parameters: \n",
    "\n",
    "        Warmup_initial_lr: 1e-6\n",
    "        Cosine_final_lr_ratio: 0.1 \n",
    "        Optimizer: Adam \n",
    "        Lr_warmup_epochs: 3 \n",
    "        Warmup_mode: linear_epoch_step \n",
    "        Ema: true \n",
    "        Delete average_best_models line \n",
    "        Ema_params: decay: 0.9, decay_type: threshold \n",
    "        Max_epochs: 25 \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    \"warmup_initial_lr\": 1e-6, # updated\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1, # updated \n",
    "    \"optimizer\": \"Adam\",# updated \n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"lr_warmup_epochs\": 3,# updated \n",
    "    \"warmup_mode\": \"linear_epoch_step\",# updated \n",
    "    \"optimizer_params\": {\"weight_decay\": 0.0001},\n",
    "    \"ema\": True,# updated \n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},# updated \n",
    "    \"max_epochs\": 25,# updated \n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\": PPYoloELoss(use_static_assigner=False, num_classes=NUM_CLASSES, reg_max=16),\n",
    "    \"valid_metrics_list\": [\n",
    "        DetectionMetrics_050(\n",
    "            score_thres=0.1,\n",
    "            top_k_predictions=300,\n",
    "            num_cls=NUM_CLASSES,\n",
    "            normalize_targets=True,\n",
    "            include_classwise_ap=True,\n",
    "            class_names=CLASS_NAMES,\n",
    "            post_prediction_callback=PPYoloEPostPredictionCallback(score_threshold=0.01, nms_top_k=1000, max_predictions=300, nms_threshold=0.7),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": \"mAP@0.50\",\n",
    "}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate YoloNAS model and launch training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    KEEP IN MIND: The training and post-training quantization must be done in the same Jupyter Notebook session. \n",
    "    Do not restart the kernel in between these two steps or PTQ will not run. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from super_gradients.training import Trainer\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "import os \n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "    For  the Trainer: \n",
    "        The experiment_name can be anything you want. \n",
    "        ckpt_root_dir  can be anything you want. Typically it is called “checkpoints.” \n",
    "        No need to create any directories; these parameters define where the trained model data will be saved. \n",
    "\"\"\"\n",
    "trainer = Trainer(experiment_name=\"vehicle_training\", ckpt_root_dir=\"checkpoints\") #update\n",
    "\n",
    "\n",
    "model = models.get('yolo_nas_s', num_classes=NUM_CLASSES, pretrained_weights=\"coco\")\n",
    "trainer.train(model=model, training_params=train_params, train_loader=train_loader, valid_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Take a second to locate your trained model. It should be in your checkpoints/experiment_name/RUN_####. \n",
    "    You should see various data, including a file called ckpt_best.pth. This is the file we will be focusing on. \n",
    "    The average_model.pth and ckpt_latest.pth gives the average weights and latest epoch, not the “best” model. \n",
    "\n",
    "    Update CKPT_PATH to match. \n",
    "\n",
    "\"\"\"\n",
    "CKPT_PATH = f'{HOME}/checkpoints/vehicle_images/RUN_20240626_101822_978143/ckpt_best.pth'\n",
    "best_model = models.get(Models.YOLO_NAS_S, num_classes=NUM_CLASSES, checkpoint_path=CKPT_PATH)\n",
    "regular_metrics = trainer.test(model=best_model, test_loader=valid_loader)\n",
    "regular_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-training quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update image path to the image you'd like to perform inference on \n",
    "prediction = best_model.predict(f'{HOME}/vehicle_images/frame_0182.png', fuse_model=False)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "    This PTQ step may not work if the training was done in a separate session, \n",
    "    or if you restarted the kernel in between the training/this step. This step should generate \n",
    "    an onnx model. After running, take a second to locate the file. Should be in your checkpoints directory. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from super_gradients.conversion import ExportParams\n",
    "\n",
    "best_model = models.get(Models.YOLO_NAS_S, num_classes=NUM_CLASSES, checkpoint_path=CKPT_PATH)\n",
    "\n",
    "export_params = ExportParams(batch_size=1, preprocessing=True, postprocessing=True)\n",
    "\n",
    "ptq_result = trainer.ptq(model=best_model, calib_loader=valid_loader, valid_loader=valid_loader, export_params=export_params)\n",
    "ptq_metrics = ptq_result.valid_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect metrics of model after PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with PTQ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update image path to match \n",
    "prediction = ptq_result.quantized_model.predict(f'{HOME}/vehicle_images/frame_0182.png', fuse_model=False)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using exported ONNX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.utils.detection_utils import DetectionVisualization\n",
    "from super_gradients.training.utils.media.image import load_image\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from super_gradients.inference import iterate_over_detection_predictions_in_batched_format\n",
    "\n",
    "\n",
    "def show_predictions_from_batch_format(image, predictions=None):\n",
    "    image_index, pred_boxes, pred_scores, pred_classes = next(iter(iterate_over_detection_predictions_in_batched_format(predictions)))\n",
    "\n",
    "    predicted_boxes = np.concatenate([pred_boxes, pred_scores[:, np.newaxis], pred_classes[:, np.newaxis]], axis=1)\n",
    "\n",
    "    image = DetectionVisualization.visualize_image(image_np=np.array(image), class_names=CLASS_NAMES, pred_boxes=predicted_boxes)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Update path to onnx file \n",
    "session = onnxruntime.InferenceSession(f'{HOME}/checkpoints/vehicle_images/ptq.onnx', providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "\n",
    "# Update path to image file \n",
    "image = load_image(f'{HOME}/padded_images/frame_0182.jpg')\n",
    "image = cv2.resize(image, (640, 640)) # update to match model input dimensions \n",
    "\n",
    "input_image = np.moveaxis(image, -1, 0)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "\n",
    "predictions1 = session.run(outputs, {inputs[0]: input_image})\n",
    "show_predictions_from_batch_format(image, predictions1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with OpenVino IR \n",
    "Based off of this tutorial https://docs.openvino.ai/2023.3/notebooks/004-hello-detection-with-output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23215"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "# These lines have changed from the original tutorial \n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Modify to point to path of your xml file \n",
    "model_xml_path = f'{HOME}/checkpoints/vehicle_images/ptq.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff724d2ccc0c4000922baf20ca5a7b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device\n",
    "\n",
    "\"\"\"\n",
    "    Can run cell and select device you'd like to use for inference \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "model = core.read_model(model=model_xml_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value) # Updated from original code to use device.value \n",
    "\n",
    "\"\"\" \n",
    "    In the tutorial, they write: \n",
    "        output_layer_ir = compiled_model.output(\"boxes\")\n",
    "    However, this output value of \"boxes\" is not universal to all models. Test with my code below, but if it is \n",
    "    not working or recognizing the output value, follow the instructions below to see what your output value is. \n",
    "        \n",
    "\t-Comment out the last three lines so there are no errors displayed. \n",
    "    -Type print(model) and look at the results. You should see a list of ConstOutput values. \n",
    "    -My model had 4 outputs: graph2_num_predictions, graph2_pred_boxes, graph2_pred_scores, and graph2_pred_classes. \n",
    "    -Since we want to draw a bounding box around objects in an image, I used “graph2_pred_boxes.” \n",
    "    -I changed the code as follows: Output_layer_ir = compiled_model.output(“graph2_pred_boxes”)\n",
    "    -I also noticed that the prediction scores were in the output and extracted those so we can mark our image with the prediction scores as well. I added in this line: \n",
    "    -Output_layer_scores = compiled_model.output(“graph2_pred_scores”)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(\"graph2_pred_boxes\")\n",
    "output_layer_scores = compiled_model.output(\"graph2_pred_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update image path to match  \n",
    "image = cv2.imread(f'{HOME}/padded_images/frame_0182.jpg')\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width.\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "#Resize the image to meet network expected input sizes.\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Above “boxes”, I added in: \n",
    "        Pred_scores = compiled_model([input_image])[output_layer_scores] from above where I extracted the output scores. \n",
    "    \n",
    "    The second line, boxes = boxes[~np.all(boxes == 0, axis=1)], caused an error. \n",
    "    This line is intended to remove “zero-only boxes.” However, in addition to this line not working, \n",
    "    my “null” predictions were marked with -1, not 0. Therefore, I modified this line to be: \n",
    "        Boxes = np.array([box for box in boxes[0] if not np.all(box == -1)])\n",
    "\n",
    "    I also replicated this for pred_scores: \n",
    "    pred_scores= np.array([score for score in pred_scores[0] if not np.all(score == -1)])\n",
    "\n",
    "    \n",
    "    If both 0 and -1 are causing you issues, print(boxes) to see if there is another value that may \n",
    "    signify null/0/-1 boxes. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create an inference request.\n",
    "pred_scores = compiled_model([input_image])[output_layer_scores]\n",
    "\n",
    "boxes = compiled_model([input_image])[output_layer_ir]\n",
    "\n",
    "# # Remove zero only boxes.\n",
    "boxes = np.array([box for box in boxes[0] if not np.all(box == -1)])  # Assuming single batch\n",
    "pred_scores = np.array([score for score in pred_scores[0] if not np.all(score == -1)])  # Assuming single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualize results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    This object detection tutorial assumekd that the inference request returns the prediction scores and the \n",
    "    bounding box coordinates in the same call. However, in this model, it actually returned pred scores and \n",
    "    bounding boxes as two separate outputs. So, the below function is modified from the original tutorial to \n",
    "    match the model output format. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# added pred_scores as a paramerer \n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, pred_scores, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        bgr_image.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Updated for-loop to loop through index\n",
    "    for i in range(0, len(boxes)):\n",
    "        \n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = pred_scores[i] \n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            \n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(boxes[i])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\")\n",
    "img = convert_result_to_image(resized_image, resized_image, boxes, pred_scores, conf_labels=True)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
