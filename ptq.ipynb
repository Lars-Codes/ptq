{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['vehicle']\n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:38:32] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /home/lpalombi/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:38:35] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: protobuf==3.20.2 does not satisfy requirement protobuf==3.20.3\u001b[0m\n",
      "[2024-07-11 11:38:35] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
      "Indexing dataset annotations: 100%|██████████| 441/441 [00:00<00:00, 7207.49it/s]\n",
      "[2024-07-11 11:38:35] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing dataset annotations: 100%|██████████| 111/111 [00:00<00:00, 10115.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters \n",
    "from super_gradients.training.datasets.detection_datasets.coco_format_detection import COCOFormatDetectionDataset\n",
    "from super_gradients.training.transforms.transforms import (\n",
    "    DetectionRandomAffine,\n",
    "    DetectionHSV,\n",
    "    DetectionHorizontalFlip,\n",
    "    DetectionPaddedRescale,\n",
    "    DetectionStandardize,\n",
    "    DetectionTargetsFormatTransform,\n",
    ")\n",
    "from super_gradients.training.utils.collate_fn import DetectionCollateFN\n",
    "\n",
    "train_dataset_params = dict(\n",
    "    data_dir=\"vehicle_images\", # local path to directory that contains both images and json annotation file\n",
    "    images_dir=\"\", # Local path FROM DATA DIR to where images are located (data_dir/path/to/images)\n",
    "    json_annotation_file=\"train.json\", # Local path FROM DATA DIR to where train.json is located\n",
    "    input_dim=(1920, 1088),\n",
    "    ignore_empty_annotations=False,\n",
    "    with_crowd=False,\n",
    "    all_classes_list=CLASS_NAMES,\n",
    "    transforms=[\n",
    "        DetectionRandomAffine(degrees=0.0, scales=(0.5, 1.5), shear=0.0, target_size=(640, 640), filter_box_candidates=False, border_value=128), # update target_size if necessary\n",
    "        DetectionHSV(prob=1.0, hgain=5, vgain=30, sgain=30),\n",
    "        DetectionHorizontalFlip(prob=0.5),\n",
    "        DetectionPaddedRescale(input_dim=(640, 640)), # update \n",
    "        DetectionStandardize(max_value=255),\n",
    "        DetectionTargetsFormatTransform(input_dim=(640, 640), output_format=\"LABEL_CXCYWH\"), # update input_dim \n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# make the same changes above here \n",
    "valid_dataset_params = dict(\n",
    "    data_dir=\"vehicle_images\", \n",
    "    images_dir=\"\",\n",
    "    json_annotation_file=\"val.json\", # val.json, not train.json \n",
    "    input_dim=(640, 640),\n",
    "    ignore_empty_annotations=False,\n",
    "    with_crowd=False,\n",
    "    all_classes_list=CLASS_NAMES,\n",
    "    transforms=[\n",
    "        DetectionPaddedRescale(input_dim=(640, 640), max_targets=300),\n",
    "        DetectionStandardize(max_value=255),\n",
    "        DetectionTargetsFormatTransform(input_dim=(640, 640), output_format=\"LABEL_CXCYWH\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainset = COCOFormatDetectionDataset(**train_dataset_params)\n",
    "valset = COCOFormatDetectionDataset(**valid_dataset_params)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 1 # update batch size \n",
    "\n",
    "train_dataloader_params = {\n",
    "    \"shuffle\": True,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"drop_last\": True,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": DetectionCollateFN(),\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"persistent_workers\": NUM_WORKERS > 0,\n",
    "}\n",
    "\n",
    "val_dataloader_params = {\n",
    "    \"shuffle\": False,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"drop_last\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": DetectionCollateFN(),\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"persistent_workers\": NUM_WORKERS > 0,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(trainset, **train_dataloader_params)\n",
    "valid_loader = DataLoader(valset, **val_dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import DetectionMetrics_050\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback\n",
    "\n",
    "train_params = {\n",
    "    \"warmup_initial_lr\": 1e-6, # updated\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1, # updated \n",
    "    \"optimizer\": \"Adam\",# updated \n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"lr_warmup_epochs\": 3,# updated \n",
    "    \"warmup_mode\": \"linear_epoch_step\",# updated \n",
    "    \"optimizer_params\": {\"weight_decay\": 0.0001},\n",
    "    \"ema\": True,# updated \n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},# updated \n",
    "    \"max_epochs\": 25,# updated \n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\": PPYoloELoss(use_static_assigner=False, num_classes=NUM_CLASSES, reg_max=16),\n",
    "    \"valid_metrics_list\": [\n",
    "        DetectionMetrics_050(\n",
    "            score_thres=0.1,\n",
    "            top_k_predictions=300,\n",
    "            num_cls=NUM_CLASSES,\n",
    "            normalize_targets=True,\n",
    "            include_classwise_ap=True,\n",
    "            class_names=CLASS_NAMES,\n",
    "            post_prediction_callback=PPYoloEPostPredictionCallback(score_threshold=0.01, nms_top_k=1000, max_predictions=300, nms_threshold=0.7),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": \"mAP@0.50\",\n",
    "}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training import Trainer\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "import os \n",
    "\n",
    "HOME = os.getcwd()\n",
    "CHECKPOINT_DIR = f'{HOME}/vehicle_training'\n",
    "\n",
    "trainer = Trainer(experiment_name=\"vehicle_training\", ckpt_root_dir=\"checkpoints\") #update \n",
    "model = models.get('yolo_nas_s', num_classes=NUM_CLASSES, pretrained_weights=\"coco\")\n",
    "trainer.train(model=model, training_params=train_params, train_loader=train_loader, valid_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = f'{HOME}/checkpoints/vehicle_images/RUN_20240626_101822_978143/ckpt_best.pth'\n",
    "\n",
    "best_model = models.get(Models.YOLO_NAS_S, num_classes=NUM_CLASSES, checkpoint_path=CKPT_PATH)\n",
    "regular_metrics = trainer.test(model=best_model, test_loader=valid_loader)\n",
    "regular_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = best_model.predict(f'{HOME}/vehicle_images/frame_0182.png', fuse_model=False)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from super_gradients.conversion import ExportParams\n",
    "\n",
    "best_model = models.get(Models.YOLO_NAS_S, num_classes=NUM_CLASSES, checkpoint_path=CKPT_PATH)\n",
    "\n",
    "export_params = ExportParams(batch_size=1, preprocessing=True, postprocessing=True)\n",
    "\n",
    "ptq_result = trainer.ptq(model=best_model, calib_loader=valid_loader, valid_loader=valid_loader, export_params=export_params)\n",
    "ptq_metrics = ptq_result.valid_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ptq_result.quantized_model.predict(f'{HOME}/vehicle_images/frame_0182.png', fuse_model=False)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.utils.detection_utils import DetectionVisualization\n",
    "from super_gradients.training.utils.media.image import load_image\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from super_gradients.inference import iterate_over_detection_predictions_in_batched_format\n",
    "\n",
    "\n",
    "def show_predictions_from_batch_format(image, predictions=None):\n",
    "    image_index, pred_boxes, pred_scores, pred_classes = next(iter(iterate_over_detection_predictions_in_batched_format(predictions)))\n",
    "\n",
    "    predicted_boxes = np.concatenate([pred_boxes, pred_scores[:, np.newaxis], pred_classes[:, np.newaxis]], axis=1)\n",
    "\n",
    "    image = DetectionVisualization.visualize_image(image_np=np.array(image), class_names=CLASS_NAMES, pred_boxes=predicted_boxes)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "session = onnxruntime.InferenceSession(f'{HOME}/checkpoints/vehicle_images/ptq.onnx', providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "\n",
    "image = load_image(f'{HOME}/padded_images/frame_0182.jpg')\n",
    "image = cv2.resize(image, (1088, 1920))\n",
    "\n",
    "input_image = np.moveaxis(image, -1, 0)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "\n",
    "predictions1 = session.run(outputs, {inputs[0]: input_image})\n",
    "show_predictions_from_batch_format(image, predictions1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with OpenVino IR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "HOME = os.getcwd()\n",
    "model_xml_path = f'{HOME}/checkpoints/vehicle_images/ptq.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff724d2ccc0c4000922baf20ca5a7b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model: 'torch_jit'\n",
      "inputs[\n",
      "<ConstOutput: names[input] shape[1,3,1920,1088] type: u8>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[graph2_num_predictions] shape[1,1] type: i64>,\n",
      "<ConstOutput: names[graph2_pred_boxes] shape[1,..2000,4] type: f32>,\n",
      "<ConstOutput: names[graph2_pred_scores] shape[1,..2000] type: f32>,\n",
      "<ConstOutput: names[graph2_pred_classes] shape[1,..2000] type: i64>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "model = core.read_model(model=model_xml_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "\n",
    "print(model)\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(\"graph2_pred_boxes\")\n",
    "output_layer_scores = compiled_model.output(\"graph2_pred_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection models expect an image in BGR format.\n",
    "image = cv2.imread(f'{HOME}/padded_images/frame_0182.jpg')\n",
    "# image = cv2.imread(f'{HOME}/vehicle_images/frame_0000.jpg')\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width.\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "#Resize the image to meet network expected input sizes.\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference request.\n",
    "pred_scores = compiled_model([input_image])[output_layer_scores]\n",
    "\n",
    "boxes = compiled_model([input_image])[output_layer_ir]\n",
    "\n",
    "# # Remove zero only boxes.\n",
    "boxes = np.array([box for box in boxes[0] if not np.all(box == -1)])  # Assuming single batch\n",
    "pred_scores = np.array([score for score in pred_scores[0] if not np.all(score == -1)])  # Assuming single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format:\n",
    "# The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, pred_scores, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        bgr_image.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for i in range(0, len(boxes)):\n",
    "        \n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = pred_scores[i]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            \n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(boxes[i])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\")\n",
    "img = convert_result_to_image(resized_image, resized_image, boxes, pred_scores, conf_labels=True)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
